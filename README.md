# IntuitiveCare-Teste - Documenta√ß√£o T√©cnica

Este reposit√≥rio cont√©m a solu√ß√£o para o teste t√©cnico de Engenharia de Dados. O projeto consiste em um pipeline ETL (Extract, Transform, Load) para coletar, limpar, enriquecer e agregar dados financeiros e cadastrais de operadoras de planos de sa√∫de a partir do portal de dados abertos da ANS.

## üîÑ Fluxo do Projeto (Pipeline)

O pipeline executa as seguintes etapas sequencialmente:

1.  **Scraping e Identifica√ß√£o:** O sistema acessa o site da ANS e identifica os arquivos de demonstra√ß√µes cont√°beis (ZIP) mais recentes (padr√£o Trimestral).
2.  **Extra√ß√£o e Transforma√ß√£o (Stream):**
    * Baixa os arquivos ZIP utilizando *streaming* para economizar mem√≥ria.
    * L√™ o conte√∫do em mem√≥ria e aplica filtros para isolar despesas assistenciais.
    * Normaliza dados num√©ricos e datas.
3.  **Consolida√ß√£o:** Unifica os dados dos trimestres processados em um √∫nico arquivo intermedi√°rio.
4.  **Enriquecimento (Join):**
    * Baixa a base cadastral de operadoras ativas.
    * Realiza o cruzamento (Join) entre dados financeiros e cadastrais via `REG_ANS`.
5.  **Valida√ß√£o (Quality Gate):** Separa registros inv√°lidos ou inconsistentes em um arquivo de "Quarentena", mantendo a integridade cont√°bil dos dados v√°lidos.
6.  **Agrega√ß√£o e Entrega:**
    * Calcula totais, m√©dias trimestrais e desvio padr√£o.
    * Gera o arquivo final compactado `Teste_Leonardo_Ruhmann.zip`.

---

## üöÄ Como Executar o Projeto

Siga os passos abaixo para rodar o pipeline ou as an√°lises.

### Pr√©-requisitos
* **Python 3.8+** (para execu√ß√£o local do ETL).
* **Git** instalado.
* **Docker & Docker Compose** (para valida√ß√£o e analytics).

### Passo a Passo

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone https://github.com/LeonardRuhmann/IntuitiveCare-Teste.git
    cd IntuitiveCare-Teste
    ```

2.  **Execute o pipeline completo (recomendado):**
    ```bash
    ./run.sh
    ```
    Este script automatiza todo o fluxo: cria o ambiente virtual, instala depend√™ncias, executa o ETL Python, sobe o Docker com MySQL 8.0, importa os dados, roda valida√ß√µes e executa as queries anal√≠ticas.

    > **Nota:** O MySQL 8.0 roda dentro do container Docker ‚Äî n√£o √© necess√°rio instal√°-lo na m√°quina.

    O script tamb√©m aceita subcomandos para execu√ß√£o parcial:
    | Comando | Descri√ß√£o |
    | :--- | :--- |
    | `./run.sh` | Pipeline completo (ETL + Docker + Analytics) |
    | `./run.sh etl` | Apenas o pipeline Python (gera os dados em `output/`) |
    | `./run.sh docker` | Apenas Docker + Analytics (requer dados j√° gerados) |
    | `./run.sh down` | Para e remove o container Docker |

3.  **Execu√ß√£o manual (alternativa):**

    Caso prefira executar cada etapa individualmente:
    
    a. **ETL Python:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # Linux/Mac
    # .\venv\Scripts\activate # Windows
    pip install -r requirements.txt
    python -m src.main
    ```

    b. **Docker & SQL:**
    ```bash
    docker-compose up -d
    # Aguarde ~30s para o MySQL inicializar e importar os dados
    docker exec -i mysql-ans mysql -uroot -proot ans_test < sql/validate.sql
    docker exec -i mysql-ans mysql -uroot -proot ans_test < sql/queries_analytics.sql
    docker-compose down -v
    ```

4.  **Verifique os Resultados (P√≥s-Etapa 1):**
    Os arquivos gerados estar√£o na pasta `output/`:
    * `Teste_Leonardo_Ruhmann.zip` ‚Äî Arquivo final com despesas agregadas
    * `data_clean.csv` ‚Äî Dados validados (entrada para agrega√ß√£o)
    * `data_quarantine.csv` ‚Äî Registros inv√°lidos/inconsistentes (auditoria)
    * `consolidado_despesas.zip` ‚Äî Dados consolidados (intermedi√°rio)
    * `enriched_data.zip` ‚Äî Dados enriquecidos com cadastro (intermedi√°rio)

---

## üìö Documenta√ß√£o T√©cnica e Decis√µes Arquiteturais

Abaixo detalho as decis√µes de design, trade-offs escolhidos e estrat√©gias de resolu√ß√£o de problemas adotadas durante o desenvolvimento.

##  Pipeline de Extra√ß√£o e Transforma√ß√£o (ETL)

### Acesso ao Portal e Arquitetura

#### **Decis√£o 1: Separa√ß√£o de Responsabilidades (`Services`)**
Criei uma pasta `services` para isolar a l√≥gica de neg√≥cio:
* `AnsDataClient`: Acesso ao portal e download.
* `IngestionService`: Orquestra√ß√£o do processamento de arquivos.
* `ZipProcessor`, `DataConsolidator`, etc.: Implementa√ß√µes espec√≠ficas de cada etapa.

* **Justificativa:** Evita misturar a l√≥gica de orquestra√ß√£o com detalhes de implementa√ß√£o.
* **‚úÖ Pr√≥s:** C√≥digo desacoplado, f√°cil manuten√ß√£o e testabilidade.
* **‚ö†Ô∏è Contras:** Aumenta a quantidade de arquivos.

#### **Decis√£o 2: Ferramenta de Scraping (`BeautifulSoup4` vs `Selenium`)**
Optei pela biblioteca `BeautifulSoup4` (BS4) em conjunto com `Requests` ao inv√©s de Selenium ou Regex puro.
* **Justificativa (KISS):** O portal da ANS disponibiliza listagens HTML est√°ticas. Usar Selenium seria *overkill* (pesado e lento). Regex puro seria fr√°gil para tags HTML.
* **‚úÖ Pr√≥s:** Leve, r√°pido e robusto contra pequenas mudan√ßas de layout HTML.
* **‚ö†Ô∏è Contras:** N√£o funcionaria se o site dependesse de JavaScript para renderizar os links (o que n√£o √© o caso).

#### **Decis√£o 3: Detec√ß√£o de Trimestres via Regex (`re`)**
Desenvolvi uma Express√£o Regular para identificar os arquivos (m√©todo `_detect_quarter`).
* **Justificativa:** Os nomes variam muito (ex: `1T2025` vs `dados_2023_1_trim`).
* **‚úÖ Pr√≥s:** Evita *hardcoding* de nomes, tornando a solu√ß√£o flex√≠vel para diferentes anos.
* **‚ö†Ô∏è Contras:** Exige uma regex complexa e bem testada para cobrir todos os casos de borda.

#### **Decis√£o 4: Download em Stream (`shutil.copyfileobj`)**
Utilizei `stream=True` nas requisi√ß√µes HTTP.
* **Justificativa:** Processa o download em partes (*chunks*).
* **‚úÖ Pr√≥s:** Previne estouro de mem√≥ria (RAM) ao baixar arquivos ZIP grandes em servidores modestos.
* **‚ö†Ô∏è Contras:** Nenhum significativo para este contexto.

#### **Decis√£o 4b: Observabilidade (Logging)**
Substitui√ß√£o de `print()` por `logging` estruturado.
* **Justificativa:** Permite monitoramento adequado em produ√ß√£o e categoriza√ß√£o de n√≠veis de erro (`INFO`, `WARNING`, `ERROR`).
* **‚úÖ Pr√≥s:** Logs mais limpos e possibilidade de persist√™ncia em arquivo.

---

###  Processamento e Transforma√ß√£o

#### **Decis√£o 5: Processamento Incremental (Iterativo)**
O script itera sobre os arquivos ZIP baixados um por um, descartando os dados brutos da mem√≥ria ap√≥s a extra√ß√£o.
* **Justificativa:** Escalabilidade.
* **‚úÖ Pr√≥s:** Se a ANS liberar arquivos de 10GB no futuro, essa solu√ß√£o continuar√° funcionando sem estourar a mem√≥ria.
* **‚ö†Ô∏è Contras:** Pode ser ligeiramente mais lento que carregar tudo na mem√≥ria para volumes muito pequenos (overhead de I/O), mas a seguran√ßa compensa.

#### **Decis√£o 6: Filtragem em Mem√≥ria (`ZipFile`)**
Uso da classe `ZipProcessor` para inspecionar o conte√∫do do ZIP sem extra√≠-lo para o disco.
* **Justificativa:** Evitar I/O desnecess√°rio de disco.
* **‚úÖ Pr√≥s:** Mais r√°pido e limpo (n√£o deixa "lixo" tempor√°rio no disco).
* **‚ö†Ô∏è Contras:** Exige manipula√ß√£o de bytes em mem√≥ria.

#### **Decis√£o 7: Normaliza√ß√£o de Encoding e Num√©ricos**
Leitura for√ßada em `UTF-8` e convers√£o de `1.000,00` para `1000.0`.
* **Justificativa:** Garantir integridade de acentos e c√°lculos matem√°ticos corretos.
* **‚úÖ Pr√≥s:** Previne erros silenciosos em an√°lises futuras.

---

###  Consolida√ß√£o e Limpeza

#### **Decis√£o 8: Tratamento de Valores Zerados vs. Negativos**
Remo√ß√£o f√≠sica de registros com valor `0.0`. O arquivo consolidado √© persistido em `output/consolidado_despesas.zip` e reutilizado nas etapas seguintes.
 **Justificativa:** Registros zerados indicam inatividade e geram "ru√≠do".
* **‚úÖ Pr√≥s:** Redu√ß√£o dr√°stica do volume de dados sem perda de informa√ß√£o.
* **‚ö†Ô∏è Contras:** Perde-se o hist√≥rico de que a conta existiu naquele trimestre (embora inativa).

> **Nota sobre Negativos:** Valores negativos foram **mantidos**, pois representam estornos ou ajustes cont√°beis leg√≠timos.

#### **Decis√£o 9: Parser de Datas (`Pandas` vs `Regex`)**
Uso de `pd.to_datetime(errors='coerce')` em vez de Regex manual.
* **Justificativa:** *Fail-safe*. O m√©todo nativo gerencia varia√ß√µes (`/` ou `-`) automaticamente.
* **‚úÖ Pr√≥s:** Robustez. Dados inv√°lidos viram `NaT` sem quebrar o pipeline.
* **‚ö†Ô∏è Contras:** Menos controle granular sobre formatos ex√≥ticos (mas suficiente para o padr√£o ANS).

---

##  Enriquecimento e Qualidade de Dados

###  Valida√ß√£o (Quarantine Pattern)

#### **Decis√£o 10: Segrega√ß√£o (Quarentena) vs. Exclus√£o**
Ao encontrar registros inv√°lidos (ex: CNPJ incorreto ou Raz√£o Social vazia), a decis√£o foi **n√£o excluir**, mas separar em `data_quarantine.csv`.
* **Justificativa:** Em sistemas financeiros, o valor monet√°rio √© real e precisa ser contabilizado, mesmo que o dado cadastral esteja errado. Excluir geraria "furos" no balan√ßo.
* **‚úÖ Pr√≥s:**
    * Preserva√ß√£o da integridade cont√°bil (Soma total bate com a origem).
    * Rastreabilidade para corre√ß√£o manual posterior (Backoffice).
* **‚ö†Ô∏è Contras:**
    * Aumenta a complexidade do pipeline (gera 2 sa√≠das em vez de 1).
    * Requer armazenamento para dados "sujos".

---

###  Enriquecimento de Dados (Cadastral)

#### **Decis√£o 11: Estrat√©gia de Join (In-Memory)**
Para cruzar dados financeiros e cadastrais, optou-se pelo processamento em mem√≥ria com Pandas, diferindo da extra√ß√£o incremental.
* **Justificativa:** O dataset final consolidado (~44k linhas) √© pequeno o suficiente para mem√≥ria RAM.
* **‚úÖ Pr√≥s (KISS):** Simplicidade e rapidez de implementa√ß√£o. Frameworks distribu√≠dos (Spark) seriam *over-engineering*.
* **‚ö†Ô∏è Contras:** Se o dataset final crescesse para a casa dos Gigabytes, essa etapa precisaria ser refatorada para chunks.

#### **Decis√£o 12: Chave de Liga√ß√£o (`REG_ANS`)**
Uso de `REG_ANS` como chave prim√°ria de join, ao inv√©s do CNPJ solicitado.
* **Justificativa (Realidade vs Requisito):** Os arquivos financeiros brutos **n√£o continham CNPJ**, apenas `REG_ANS`.
* **‚úÖ Pr√≥s:** Viabilizou o enriquecimento. O CNPJ foi trazido da base cadastral para a financeira.
* **‚ö†Ô∏è Contras:** Depend√™ncia da qualidade da base cadastral da ANS.

---

###  Resolu√ß√£o de Anomalias

Durante a valida√ß√£o, foram tomadas decis√µes espec√≠ficas para "Sanitizar" os dados:

1.  **Bug da "SUL AMERICA" (Zeros √† Esquerda)**
    * **Problema:** O CSV da ANS trazia CNPJs como num√©ricos (`1685...` - 13 d√≠gitos), causando falha na valida√ß√£o.
    * **A√ß√£o:** Implementa√ß√£o de `zfill(14)` no `DataEnricher`.
    * **Resultado:** Taxa de aprova√ß√£o subiu de 3% para 99%.

2.  **Registro sem Match (Ghost)**
    * **A√ß√£o:** `Left Join`.
    * **Justificativa:** Mant√©m a despesa financeira mesmo se a operadora n√£o for encontrada no cadastro ativo. Prioridade √© o dado financeiro.

3.  **Duplicidade no Cadastro**
    * **A√ß√£o:** Deduplica√ß√£o pr√©via por `REGISTRO_OPERADORA`.
    * **Justificativa:** Evita a explos√£o de linhas (Produto Cartesiano) no join 1:N.

###  Estrat√©gia de Agrega√ß√£o e Ordena√ß√£o

Para a gera√ß√£o do relat√≥rio final, optou-se pelo uso de **Processamento em Mem√≥ria (In-Memory)** utilizando a biblioteca Pandas.

* **Estrat√©gia:** O c√°lculo de m√©tricas (Soma, Desvio Padr√£o e M√©dia Trimestral) e a ordena√ß√£o final foram realizados carregando o dataset consolidado na mem√≥ria RAM.
* **Algoritmo de Ordena√ß√£o:** Utilizou-se o m√©todo `sort_values` do Pandas, que implementa uma varia√ß√£o otimizada do *Quicksort* (Complexidade m√©dia $O(N \log N)$).
* **Justificativa do Trade-off:**
    * **Volume de Dados vs. Complexidade:** O volume total de dados processados resulta em um dataframe de baixo consumo de mem√≥ria (< 100MB). Implementar algoritmos de ordena√ß√£o externa (*External Merge Sort*) ou utilizar processamento distribu√≠do (Spark) adicionaria complexidade de infraestrutura desnecess√°ria (*Over-engineering*) para o escopo atual.
    * **Performance:** A opera√ß√£o em mem√≥ria elimina o *overhead* de I/O de disco, resultando em um tempo de execu√ß√£o de milissegundos para a etapa de agrega√ß√£o.

---

## ÔøΩÔ∏è Banco de Dados e Queries Anal√≠ticas

Esta se√ß√£o documenta as decis√µes de modelagem do banco de dados (`sql/ddl_schema.sql`), a estrat√©gia de importa√ß√£o (`sql/import_data.sql`) e as queries anal√≠ticas (`sql/queries_analytics.sql`).

### Modelagem do Esquema (DDL)

#### **Decis√£o 13: Normaliza√ß√£o ‚Äî Star Schema (Op√ß√£o B)**

> **Trade-off t√©cnico:** Tabela desnormalizada (Op√ß√£o A) vs. Tabelas normalizadas separadas (Op√ß√£o B).

Optou-se pela **Op√ß√£o B: Tabelas normalizadas** seguindo o modelo *Star Schema*, com prefixos `dim_` (dimens√£o) e `fact_` (fato):

| Tabela | Tipo | Fun√ß√£o |
| :--- | :--- | :--- |
| `dim_operadoras` | Dimens√£o | Dados cadastrais √∫nicos por operadora |
| `fact_despesas_eventos` | Fato | Registros financeiros trimestrais |

* **Justificativa:**
    * **Volume de dados esperado:** A tabela fato cresce a cada trimestre, enquanto a dimens√£o √© est√°vel. Duplicar dados cadastrais em cada linha da fato seria desperd√≠cio.
    * **Frequ√™ncia de atualiza√ß√µes:** Dados cadastrais (raz√£o social, UF) mudam raramente. Com normaliza√ß√£o, uma atualiza√ß√£o no cadastro propaga automaticamente para todas as queries sem alterar a fato.
    * **Complexidade das queries:** O `JOIN` entre `dim` e `fact` via `reg_ans` √© simples e indexado, sem impacto percept√≠vel em performance.
* **‚úÖ Pr√≥s:** Elimina√ß√£o de redund√¢ncia, integridade referencial via Foreign Keys, e atualiza√ß√£o √∫nica de dados cadastrais.
* **‚ö†Ô∏è Contras:** Requer `JOIN` em todas as queries anal√≠ticas (custo neglig√≠vel para este volume).

#### **Decis√£o 14: Tipos de Dados ‚Äî Precis√£o vs. Performance**

> **Trade-off t√©cnico:** Para valores monet√°rios: `DECIMAL` vs `FLOAT` vs `INTEGER` (centavos). Para datas: `DATE` vs `VARCHAR` vs `TIMESTAMP`.

**Valores Monet√°rios ‚Üí `NUMERIC(18,2)`**
* **Justificativa:** Dados financeiros exigem precis√£o exata. `FLOAT`/`DOUBLE` usam representa√ß√£o bin√°ria de ponto flutuante (IEEE 754) e introduzem erros de arredondamento silenciosos (ex: `0.1 + 0.2 = 0.30000000000000004`). `INTEGER` em centavos √© v√°lido, mas exige convers√£o constante na camada de aplica√ß√£o e dificulta a leitura direta no banco.
* **‚úÖ Pr√≥s:** Zero perda de precis√£o. Opera√ß√µes `SUM()` e `AVG()` retornam valores exatos.
* **‚ö†Ô∏è Contras:** Ligeiramente mais lento que `FLOAT` em opera√ß√µes massivas, mas irrelevante para o nosso volume.

**Datas ‚Üí `DATE`**
* **Justificativa:** Permite opera√ß√µes temporais nativas como `MIN()`, `MAX()` e compara√ß√µes por igualdade (`WHERE data_trimestre = ...`), essenciais para a l√≥gica de *Snapshot Final* e pivot trimestral. `VARCHAR` exigiria convers√£o em cada query e impediria ordena√ß√£o/compara√ß√£o correta. `TIMESTAMP` adicionaria hora/minuto/segundo sem utilidade para dados trimestrais.
* **‚úÖ Pr√≥s:** Indexa√ß√£o eficiente, compara√ß√µes nativas e convers√£o via `STR_TO_DATE()` na importa√ß√£o.
* **‚ö†Ô∏è Contras:** Nenhum significativo para este contexto.

#### **Decis√£o 15: Estrat√©gia de Indexa√ß√£o**

Foram criados √≠ndices espec√≠ficos para otimizar as queries anal√≠ticas mais frequentes:

| √çndice | Coluna | Justificativa |
| :--- | :--- | :--- |
| `idx_operadoras_cnpj` | `dim_operadoras.cnpj` | Preparado para extensibilidade (lookups futuros por CNPJ). N√£o utilizado nas queries atuais. |
| `idx_despesas_data` | `fact_despesas_eventos.data_trimestre` | Acelera `MIN()`, `MAX()` e compara√ß√µes por igualdade (`WHERE data_trimestre = ...`) |
| `idx_despesas_reg` | `fact_despesas_eventos.reg_ans` | `JOIN` com a dimens√£o e agrupamentos (`GROUP BY`) |

* **Justificativa:** Sem √≠ndices, toda query anal√≠tica faria *Full Table Scan*. Com o crescimento da tabela fato, isso se tornaria invi√°vel.
* **‚ö†Ô∏è Trade-off:** √çndices aceleram leituras (`SELECT`) mas desaceleram escritas (`INSERT`). Como a importa√ß√£o ocorre em *batch* (uma vez por trimestre), o custo de escrita √© aceit√°vel.

---

### Importa√ß√£o de Dados (ETL SQL)

#### **Decis√£o 16: Staging Tables (Tabelas Tempor√°rias)**

Utilizou-se a abordagem de **tabelas tempor√°rias** (`CREATE TEMPORARY TABLE`) como √°rea de *staging* para receber os dados brutos do CSV antes de transform√°-los e inseri-los nas tabelas finais.

* **Justificativa:** Os CSVs da ANS cont√™m dados em formatos incompat√≠veis com o schema final (datas como `VARCHAR`, valores monet√°rios com v√≠rgula, colunas extras desnecess√°rias).
* **‚úÖ Pr√≥s:** Permite transforma√ß√£o (`STR_TO_DATE`, `REPLACE`, `CAST`) em SQL puro sem depend√™ncia de ferramentas externas.
* **‚ö†Ô∏è Contras:** Consome mem√≥ria tempor√°ria do servidor durante a importa√ß√£o.

#### **Decis√£o 17: Mapeamento Completo de Colunas (vs. `@dummy`)**

A tabela `temp_operadoras` reflete **todas as 20 colunas** do CSV original (`Relatorio_cadop.csv`), mesmo que apenas 5 sejam utilizadas.

* **Justificativa (Clean Code):** Usar `@dummy` para 15 colunas torna o c√≥digo ileg√≠vel e imposs√≠vel de auditar. Com o mapeamento completo, o script funciona como **documenta√ß√£o viva** da estrutura do arquivo fonte.
* **‚úÖ Pr√≥s:** Autodocumentado, extens√≠vel (se precisar de uma nova coluna, basta adicion√°-la ao `INSERT`).
* **‚ö†Ô∏è Contras:** A tabela tempor√°ria ocupa mais mem√≥ria, mas √© descartada imediatamente ap√≥s o uso.

#### **An√°lise Cr√≠tica: Tratamento de Inconsist√™ncias na Importa√ß√£o**

| Inconsist√™ncia | Estrat√©gia | Justificativa |
| :--- | :---: | :--- |
| **Valores NULL em campos obrigat√≥rios** | Rejei√ß√£o via `WHERE` | Registros sem `reg_ans` s√£o filtrados pelo `WHERE reg_ans IN (...)`, pois `NULL` nunca satisfaz a condi√ß√£o `IN`. Garante integridade referencial. |
| **Strings em campos num√©ricos** | Convers√£o com `REPLACE` + `CAST` | A v√≠rgula decimal (`1234,56`) √© convertida para ponto (`1234.56`) via `REPLACE`. Se a convers√£o falhar, o `CAST` retorna `NULL`, isolando o erro sem quebrar o batch. |
| **Datas em formatos inconsistentes** | `STR_TO_DATE` com formato expl√≠cito | For√ßa o padr√£o `%Y-%m-%d`. Datas fora deste formato retornam `NULL` e s√£o tratadas pelo `NOT NULL` constraint na tabela final, rejeitando o registro. |
| **Operadoras duplicadas no CSV** | `INSERT IGNORE` + `SELECT DISTINCT` | O `DISTINCT` elimina duplicatas na leitura; o `INSERT IGNORE` garante idempot√™ncia caso a mesma operadora j√° exista na tabela. |
| **Encoding incorreto** | `CHARACTER SET 'utf8'` | For√ßamos UTF-8 na leitura para preservar acentos e caracteres especiais. |

---

### Queries Anal√≠ticas

#### **Decis√£o 18: Exclus√£o de Operadoras com Valores Zero**

> **Desafio:** Considerar operadoras que podem n√£o ter dados em todos os trimestres.

Na Query 1 (Crescimento %), optou-se por **excluir operadoras cujo valor no primeiro trimestre √© zero** (`WHERE q1_ytd > 0`).

* **Justificativa:** Valor zero no trimestre inicial indica inatividade ou aus√™ncia de registro. Calcular crescimento percentual a partir de zero resultaria em divis√£o por zero ou crescimento infinito, distorcendo o ranking.
* **‚úÖ Pr√≥s:** Resultados matematicamente v√°lidos e representativos do mercado ativo.
* **‚ö†Ô∏è Contras:** Operadoras que **iniciaram** atividade durante o per√≠odo analisado n√£o aparecem no ranking de crescimento, mesmo que tenham valores expressivos no √∫ltimo trimestre.

> **Limita√ß√£o conhecida ‚Äî Continuidade de dados:** A query n√£o valida se h√° dados intermedi√°rios entre o primeiro e o √∫ltimo trimestre. Uma operadora com dados apenas em Q1 e Q3 (sem Q2) teria seu crescimento calculado normalmente, mesmo que o gap indique suspens√£o de opera√ß√µes, fus√£o/cis√£o ou problemas de qualidade de dados. Para o escopo atual (3 trimestres, dados regulados da ANS), o risco √© baixo. Em um sistema de produ√ß√£o com hist√≥rico de 5+ anos, seria recomend√°vel implementar um filtro de completude m√≠nima (ex: dados em ‚â• 80% dos trimestres esperados).

#### **Decis√£o 19: Otimiza√ß√£o de Subqueries ‚Äî CTE `DateBounds`**

Na Query 1, as subqueries `SELECT MIN(data_trimestre)` e `SELECT MAX(data_trimestre)` eram executadas **inline** dentro de express√µes `CASE WHEN`, potencialmente recalculadas para cada linha do `GROUP BY`.

Refatorou-se para uma **CTE preliminar** (`DateBounds`) que calcula os limites uma √∫nica vez e √© referenciada via `CROSS JOIN`.

* **Justificativa:** Boa pr√°tica de performance independente do volume. Elimina scans redundantes na tabela fato.
* **‚úÖ Pr√≥s:** O otimizador calcula MIN/MAX uma vez; c√≥digo mais limpo e expl√≠cito.
* **‚ö†Ô∏è Contras:** Nenhum. O `CROSS JOIN` com uma CTE de 1 linha n√£o adiciona custo.

#### **Decis√£o 20: CTEs com Flags (vs. Window Functions vs. Subqueries)**

> **Trade-off t√©cnico:** A Query 3 (Operadoras acima da m√©dia em ‚â• 2 trimestres) pode ser resolvida com diferentes abordagens.

Optou-se pela **Abordagem A: CTEs (Common Table Expressions) com Flags**, ao inv√©s de Window Functions ou Subqueries correlacionadas.

| Crit√©rio | CTEs (Escolhida) | Window Functions | Subqueries |
| :--- | :---: | :---: | :---: |
| **Legibilidade** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê |
| **Manutenibilidade** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê |
| **Performance** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê |



* **Justificativa:**
    * **Legibilidade:** Cada CTE (`MarketAverage`, `OperatorYTD`, `AboveAverage`) tem uma responsabilidade √∫nica e nomeada, funcionando como "etapas" de um pipeline l√≥gico.
    * **Manutenibilidade:** Alterar o limiar de 2 para 3 trimestres exige mudar apenas `HAVING SUM(...) >= 3`.
    * **Performance:** Window Functions leem a tabela fato apenas 1 vez, mas a clareza do c√≥digo foi priorizada.

---

### üêõ A Anomalia dos "71 Bilh√µes" (e a Solu√ß√£o SQL)

**O Problema:**
Ao realizar a primeira agrega√ß√£o dos arquivos trimestrais (`1T2025.csv`, `2T2025.csv`, `3T2025.csv`), o pipeline reportou um total de despesas para a operadora *Bradesco Sa√∫de* superior a **R$ 71 Bilh√µes**. Este valor representava uma anomalia estat√≠stica quando comparado ao hist√≥rico de mercado da empresa.

**Investiga√ß√£o (RCA - Root Cause Analysis):**
A an√°lise explorat√≥ria revelou que os arquivos de Demonstra√ß√µes Cont√°beis da ANS seguem o regime de **compet√™ncia acumulada (Year-to-Date / YTD)**.
* *Evid√™ncia:* A conta `411111061` (Despesas com Eventos) apresentava saldo crescente sem "zerar" a cada trimestre.
* *Erro Original:* A estrat√©gia inicial de somar (`SUM`) os valores de todos os arquivos resultava na duplica√ß√£o (e triplica√ß√£o) dos saldos dos primeiros meses do ano.

**Solu√ß√£o Implementada (Estrat√©gia H√≠brida):**
Para garantir a integridade dos n√∫meros tanto na agrega√ß√£o Python quanto na an√°lise SQL, adotamos abordagens complementares:

1.  **No Pipeline Python (Data Aggregator):**
    Utilizamos a **Snapshot Strategy** para m√©tricas de volume. O c√≥digo filtra o dataset pelo `MAX(DATA)` (√∫ltimo trimestre) antes de realizar a soma, corrigindo o total exportado para o valor real de **~R$ 36.5 Bilh√µes**.

2.  **Nas Queries SQL (Analytics):**
    Implementamos a l√≥gica de **Desacumula√ß√£o Incremental** diretamente no banco de dados para an√°lises temporais:
    *   **Crescimento Real (Query 1):** Ao inv√©s de comparar acumulados (que distorceriam o percentual), extra√≠mos o valor real do trimestre:
        *   `Q3_Real = Q3_YTD - Q2_YTD`
        *   `Crescimento = (Q3_Real - Q1_Real) / Q1_Real`
    *   **Volume Total (Query 2):** Utilizamos o *Snapshot Final* (`MAX(data_trimestre)`), alinhado com a l√≥gica do Python.

3.  **Valida√ß√£o Automatizada:**
    Criamos o script `sql/validate.sql` que verificou a hierarquia de contas, confirmando que os dados consolidados s√£o puramente **Level 9 (Anal√≠ticos/Leaf)**, eliminando a hip√≥tese de dupla contagem por hierarquia.

> **Status:** ‚úÖ Resolvido em Python (`fix/aggregation-ytd-logic`) e SQL (`feat/ytd-analytics`).
