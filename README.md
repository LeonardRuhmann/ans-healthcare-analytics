# IntuitiveCare-Teste - Documenta√ß√£o T√©cnica

Este reposit√≥rio cont√©m a solu√ß√£o para o teste t√©cnico de Engenharia de Dados. O projeto consiste em um pipeline ETL (Extract, Transform, Load) para coletar, limpar, enriquecer e agregar dados financeiros e cadastrais de operadoras de planos de sa√∫de a partir do portal de dados abertos da ANS.

## üîÑ Fluxo do Projeto (Pipeline)

O pipeline executa as seguintes etapas sequencialmente:

1.  **Scraping e Identifica√ß√£o:** O sistema acessa o site da ANS e identifica os arquivos de demonstra√ß√µes cont√°beis (ZIP) mais recentes (padr√£o Trimestral).
2.  **Extra√ß√£o e Transforma√ß√£o (Stream):**
    * Baixa os arquivos ZIP utilizando *streaming* para economizar mem√≥ria.
    * L√™ o conte√∫do em mem√≥ria e aplica filtros para isolar despesas assistenciais.
    * Normaliza dados num√©ricos e datas.
3.  **Consolida√ß√£o:** Unifica os dados dos trimestres processados em um √∫nico arquivo intermedi√°rio.
4.  **Enriquecimento (Join):**
    * Baixa a base cadastral de operadoras ativas.
    * Realiza o cruzamento (Join) entre dados financeiros e cadastrais via `REG_ANS`.
5.  **Valida√ß√£o (Quality Gate):** Separa registros inv√°lidos ou inconsistentes em um arquivo de "Quarentena", mantendo a integridade cont√°bil dos dados v√°lidos.
6.  **Agrega√ß√£o e Entrega:**
    * Calcula totais, m√©dias trimestrais e desvio padr√£o.
    * Gera o arquivo final compactado `Teste_{Nome}.zip`.

---

## üöÄ Como Executar o Projeto

Siga os passos abaixo para rodar o pipeline em seu ambiente local.

### Pr√©-requisitos
* **Python 3.8+** instalado.
* **Git** instalado.

### Passo a Passo

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone [https://github.com/LeonardRuhmann/IntuitiveCare-Teste.git](https://github.com/LeonardRuhmann/IntuitiveCare-Teste.git)
    cd IntuitiveCare-Teste
    ```

2.  **Crie e ative um ambiente virtual (Recomendado):**
    * *Linux/Mac:*
        ```bash
        python3 -m venv venv
        source venv/bin/activate
        ```
    * *Windows:*
        ```bash
        python -m venv venv
        .\venv\Scripts\activate
        ```

3.  **Instale as depend√™ncias:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Execute o Pipeline:**
    ```bash
    python -m src.main
    ```

5.  **Verifique os Resultados:**
    Os arquivos gerados estar√£o na pasta `output/`:
    * `Teste_Leonardo_Ruhmann.zip` ‚Äî Arquivo final com despesas agregadas
    * `data_clean.csv` ‚Äî Dados validados (entrada para agrega√ß√£o)
    * `data_quarantine.csv` ‚Äî Registros inv√°lidos/inconsistentes (auditoria)
    * `consolidado_despesas.zip` ‚Äî Dados consolidados (intermedi√°rio)
    * `enriched_data.zip` ‚Äî Dados enriquecidos com cadastro (intermedi√°rio)

---

## üìö Documenta√ß√£o T√©cnica e Decis√µes Arquiteturais

Abaixo detalho as decis√µes de design, trade-offs escolhidos e estrat√©gias de resolu√ß√£o de problemas adotadas durante o desenvolvimento.

##  Pipeline de Extra√ß√£o e Transforma√ß√£o (ETL)

### Acesso ao Portal e Arquitetura

#### **Decis√£o 1: Separa√ß√£o de Responsabilidades (`Services`)**
Criei uma pasta `services` para isolar a l√≥gica de neg√≥cio:
* `AnsDataClient`: Acesso ao portal e download.
* `IngestionService`: Orquestra√ß√£o do processamento de arquivos.
* `ZipProcessor`, `DataConsolidator`, etc.: Implementa√ß√µes espec√≠ficas de cada etapa.

* **Justificativa:** Evita misturar a l√≥gica de orquestra√ß√£o com detalhes de implementa√ß√£o.
* **‚úÖ Pr√≥s:** C√≥digo desacoplado, f√°cil manuten√ß√£o e testabilidade.
* **‚ö†Ô∏è Contras:** Aumenta a quantidade de arquivos.

#### **Decis√£o 2: Ferramenta de Scraping (`BeautifulSoup4` vs `Selenium`)**
Optei pela biblioteca `BeautifulSoup4` (BS4) em conjunto com `Requests` ao inv√©s de Selenium ou Regex puro.
* **Justificativa (KISS):** O portal da ANS disponibiliza listagens HTML est√°ticas. Usar Selenium seria *overkill* (pesado e lento). Regex puro seria fr√°gil para tags HTML.
* **‚úÖ Pr√≥s:** Leve, r√°pido e robusto contra pequenas mudan√ßas de layout HTML.
* **‚ö†Ô∏è Contras:** N√£o funcionaria se o site dependesse de JavaScript para renderizar os links (o que n√£o √© o caso).

#### **Decis√£o 3: Detec√ß√£o de Trimestres via Regex (`re`)**
Desenvolvi uma Express√£o Regular para identificar os arquivos (m√©todo `_detect_quarter`).
* **Justificativa:** Os nomes variam muito (ex: `1T2025` vs `dados_2023_1_trim`).
* **‚úÖ Pr√≥s:** Evita *hardcoding* de nomes, tornando a solu√ß√£o flex√≠vel para diferentes anos.
* **‚ö†Ô∏è Contras:** Exige uma regex complexa e bem testada para cobrir todos os casos de borda.

#### **Decis√£o 4: Download em Stream (`shutil.copyfileobj`)**
Utilizei `stream=True` nas requisi√ß√µes HTTP.
* **Justificativa:** Processa o download em partes (*chunks*).
* **‚úÖ Pr√≥s:** Previne estouro de mem√≥ria (RAM) ao baixar arquivos ZIP grandes em servidores modestos.
* **‚ö†Ô∏è Contras:** Nenhum significativo para este contexto.

#### **Decis√£o 4b: Observabilidade (Logging)**
Substitui√ß√£o de `print()` por `logging` estruturado.
* **Justificativa:** Permite monitoramento adequado em produ√ß√£o e categoriza√ß√£o de n√≠veis de erro (`INFO`, `WARNING`, `ERROR`).
* **‚úÖ Pr√≥s:** Logs mais limpos e possibilidade de persist√™ncia em arquivo.

---

###  Processamento e Transforma√ß√£o

#### **Decis√£o 5: Processamento Incremental (Iterativo)**
O script itera sobre os arquivos ZIP baixados um por um, descartando os dados brutos da mem√≥ria ap√≥s a extra√ß√£o.
* **Justificativa:** Escalabilidade.
* **‚úÖ Pr√≥s:** Se a ANS liberar arquivos de 10GB no futuro, essa solu√ß√£o continuar√° funcionando sem estourar a mem√≥ria.
* **‚ö†Ô∏è Contras:** Pode ser ligeiramente mais lento que carregar tudo na mem√≥ria para volumes muito pequenos (overhead de I/O), mas a seguran√ßa compensa.

#### **Decis√£o 6: Filtragem em Mem√≥ria (`ZipFile`)**
Uso da classe `ZipProcessor` para inspecionar o conte√∫do do ZIP sem extra√≠-lo para o disco.
* **Justificativa:** Evitar I/O desnecess√°rio de disco.
* **‚úÖ Pr√≥s:** Mais r√°pido e limpo (n√£o deixa "lixo" tempor√°rio no disco).
* **‚ö†Ô∏è Contras:** Exige manipula√ß√£o de bytes em mem√≥ria.

#### **Decis√£o 7: Normaliza√ß√£o de Encoding e Num√©ricos**
Leitura for√ßada em `UTF-8` e convers√£o de `1.000,00` para `1000.0`.
* **Justificativa:** Garantir integridade de acentos e c√°lculos matem√°ticos corretos.
* **‚úÖ Pr√≥s:** Previne erros silenciosos em an√°lises futuras.

---

###  Consolida√ß√£o e Limpeza

#### **Decis√£o 8: Tratamento de Valores Zerados vs. Negativos**
Remo√ß√£o f√≠sica de registros com valor `0.0`. O arquivo consolidado √© persistido em `output/consolidado_despesas.zip` e reutilizado nas etapas seguintes.
 **Justificativa:** Registros zerados indicam inatividade e geram "ru√≠do".
* **‚úÖ Pr√≥s:** Redu√ß√£o dr√°stica do volume de dados sem perda de informa√ß√£o.
* **‚ö†Ô∏è Contras:** Perde-se o hist√≥rico de que a conta existiu naquele trimestre (embora inativa).

> **Nota sobre Negativos:** Valores negativos foram **mantidos**, pois representam estornos ou ajustes cont√°beis leg√≠timos.

#### **Decis√£o 9: Parser de Datas (`Pandas` vs `Regex`)**
Uso de `pd.to_datetime(errors='coerce')` em vez de Regex manual.
* **Justificativa:** *Fail-safe*. O m√©todo nativo gerencia varia√ß√µes (`/` ou `-`) automaticamente.
* **‚úÖ Pr√≥s:** Robustez. Dados inv√°lidos viram `NaT` sem quebrar o pipeline.
* **‚ö†Ô∏è Contras:** Menos controle granular sobre formatos ex√≥ticos (mas suficiente para o padr√£o ANS).

---

##  Enriquecimento e Qualidade de Dados

###  Valida√ß√£o (Quarantine Pattern)

#### **Decis√£o 10: Segrega√ß√£o (Quarentena) vs. Exclus√£o**
Ao encontrar registros inv√°lidos (ex: CNPJ incorreto ou Raz√£o Social vazia), a decis√£o foi **n√£o excluir**, mas separar em `data_quarantine.csv`.
* **Justificativa:** Em sistemas financeiros, o valor monet√°rio √© real e precisa ser contabilizado, mesmo que o dado cadastral esteja errado. Excluir geraria "furos" no balan√ßo.
* **‚úÖ Pr√≥s:**
    * Preserva√ß√£o da integridade cont√°bil (Soma total bate com a origem).
    * Rastreabilidade para corre√ß√£o manual posterior (Backoffice).
* **‚ö†Ô∏è Contras:**
    * Aumenta a complexidade do pipeline (gera 2 sa√≠das em vez de 1).
    * Requer armazenamento para dados "sujos".

---

###  Enriquecimento de Dados (Cadastral)

#### **Decis√£o 11: Estrat√©gia de Join (In-Memory)**
Para cruzar dados financeiros e cadastrais, optou-se pelo processamento em mem√≥ria com Pandas, diferindo da extra√ß√£o incremental.
* **Justificativa:** O dataset final consolidado (~44k linhas) √© pequeno o suficiente para mem√≥ria RAM.
* **‚úÖ Pr√≥s (KISS):** Simplicidade e rapidez de implementa√ß√£o. Frameworks distribu√≠dos (Spark) seriam *over-engineering*.
* **‚ö†Ô∏è Contras:** Se o dataset final crescesse para a casa dos Gigabytes, essa etapa precisaria ser refatorada para chunks.

#### **Decis√£o 12: Chave de Liga√ß√£o (`REG_ANS`)**
Uso de `REG_ANS` como chave prim√°ria de join, ao inv√©s do CNPJ solicitado.
* **Justificativa (Realidade vs Requisito):** Os arquivos financeiros brutos **n√£o continham CNPJ**, apenas `REG_ANS`.
* **‚úÖ Pr√≥s:** Viabilizou o enriquecimento. O CNPJ foi trazido da base cadastral para a financeira.
* **‚ö†Ô∏è Contras:** Depend√™ncia da qualidade da base cadastral da ANS.

---

###  Resolu√ß√£o de Anomalias

Durante a valida√ß√£o, foram tomadas decis√µes espec√≠ficas para "Sanitizar" os dados:

1.  **Bug da "SUL AMERICA" (Zeros √† Esquerda)**
    * **Problema:** O CSV da ANS trazia CNPJs como num√©ricos (`1685...` - 13 d√≠gitos), causando falha na valida√ß√£o.
    * **A√ß√£o:** Implementa√ß√£o de `zfill(14)` no `DataEnricher`.
    * **Resultado:** Taxa de aprova√ß√£o subiu de 3% para 99%.

2.  **Registro sem Match (Ghost)**
    * **A√ß√£o:** `Left Join`.
    * **Justificativa:** Mant√©m a despesa financeira mesmo se a operadora n√£o for encontrada no cadastro ativo. Prioridade √© o dado financeiro.

3.  **Duplicidade no Cadastro**
    * **A√ß√£o:** Deduplica√ß√£o pr√©via por `REGISTRO_OPERADORA`.
    * **Justificativa:** Evita a explos√£o de linhas (Produto Cartesiano) no join 1:N.

###  Estrat√©gia de Agrega√ß√£o e Ordena√ß√£o

Para a gera√ß√£o do relat√≥rio final, optou-se pelo uso de **Processamento em Mem√≥ria (In-Memory)** utilizando a biblioteca Pandas.

* **Estrat√©gia:** O c√°lculo de m√©tricas (Soma, Desvio Padr√£o e M√©dia Trimestral) e a ordena√ß√£o final foram realizados carregando o dataset consolidado (~50k registros) na mem√≥ria RAM.
* **Algoritmo de Ordena√ß√£o:** Utilizou-se o m√©todo `sort_values` do Pandas, que implementa uma varia√ß√£o otimizada do *Quicksort* (Complexidade m√©dia $O(N \log N)$).
* **Justificativa do Trade-off:**
    * **Volume de Dados vs. Complexidade:** O volume total de dados processados resulta em um dataframe de baixo consumo de mem√≥ria (< 100MB). Implementar algoritmos de ordena√ß√£o externa (*External Merge Sort*) ou utilizar processamento distribu√≠do (Spark) adicionaria complexidade de infraestrutura desnecess√°ria (*Over-engineering*) para o escopo atual.
    * **Performance:** A opera√ß√£o em mem√≥ria elimina o *overhead* de I/O de disco, resultando em um tempo de execu√ß√£o de milissegundos para a etapa de agrega√ß√£o.